{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aefc5ebd",
   "metadata": {},
   "source": [
    "# Window functions\n",
    "\n",
    "- 앞서 배웠던 pyspark.functions는 substr()과 같이 문자열을 잘라내고, 그 결과를 생성하여 행으로 적용한다.\n",
    "\n",
    "- 반면에 윈도우 함수는 그룹으로 구분하고 그 범위 내에서 계산을 할 때 사용한다.\n",
    "\n",
    "- 이 때 over() 함수로 적용되는 그룹의 윈도우를 구분하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d800eae",
   "metadata": {},
   "source": [
    "#### 순위함수\n",
    "- ranking functions : rank, dense_rank, percent_rank, ntile, row_number\n",
    "\n",
    "#### 분석 함수\n",
    "- analytic functions : cume_dist, first_value, last_value, lag, lead\n",
    "\n",
    "#### 집합 함수\n",
    "- aggregate functions : sum, avg, min, max, count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eea05ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/01 09:26:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local')\\\n",
    "    .appName('window_functions')\\\n",
    "    .config(conf=pyspark.SparkConf())\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dc990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "marks=[\n",
    "    \"김하나, English, 100\",\n",
    "    \"김하나, Math, 80\",\n",
    "    \"임하나, English, 70\",\n",
    "    \"임하나, Math, 100\",\n",
    "    \"김갑돌, English, 82.3\",\n",
    "    \"김갑돌, Math, 98.5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7381531a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['김하나', 'English', '100'],\n",
       " ['김하나', 'Math', '80'],\n",
       " ['임하나', 'English', '70'],\n",
       " ['임하나', 'Math', '100'],\n",
       " ['김갑돌', 'English', '82.3'],\n",
       " ['김갑돌', 'Math', '98.5']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_makrsRdd = spark.sparkContext.parallelize(marks).map(lambda x: x.split(', '))\n",
    "_makrsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e132bf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- mark: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_markDf = spark.createDataFrame(_makrsRdd, schema=[\"name\", \"subject\", \"mark\"])\n",
    "_markDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46916fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|subject|count|\n",
      "+-------+-----+\n",
      "|   Math|    3|\n",
      "|English|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:=============================================>          (61 + 1) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_markDf.groupBy(\"subject\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c0343",
   "metadata": {},
   "source": [
    "# Group window\n",
    "\n",
    "column에 대해 group, 즉 윈도우를 만드는 함수를 보자\n",
    "\n",
    "#### partitionBy()\n",
    "- column 별로 구분, 즉 컬럼 값에 따라 partition에 포함되는 행을 할당.\n",
    "\n",
    "- partition을 정하지 않으면 모든 행을 동일한 노드에 할당한다.\n",
    "\n",
    "#### orderBy()\n",
    "- partition 내에서 컬럼 값에 대해 행의 순서를 정렬한다.\n",
    "\n",
    "#### frame()\n",
    "- 현재 행을 기준으로 포함할 행을 분할\n",
    "\n",
    "- 행 프레임 : 현재 행을 기준으로 몇 개 앞, 몇 개 뒤의 물리적 범위를 window.rowsBetween(start, end)로 정한다.\n",
    "\n",
    "- 범위 프레임 : 논리적인 범위를 windowSpec.rangeBetween(start, end)로 정한다.\n",
    "\n",
    "- 현재 성적이 70점이면 RANGE BETWEEN 20 PERCEDING AND 10 FOLOWING 은 50 - 80을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a86bc438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "win = Window.partitionBy(\"subject\").orderBy(\"mark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e371bdd",
   "metadata": {},
   "source": [
    "### 순위 함수\n",
    "**row number**\n",
    "\n",
    "- row_number() 윈도우 함수는 각 그룹별로 일련 번호를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b5fc3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+----------+\n",
      "|  name|subject|mark|row_number|\n",
      "+------+-------+----+----------+\n",
      "|임하나|   Math| 100|         1|\n",
      "|김하나|   Math|  80|         2|\n",
      "|김갑돌|   Math|98.5|         3|\n",
      "|김하나|English| 100|         1|\n",
      "|임하나|English|  70|         2|\n",
      "|김갑돌|English|82.3|         3|\n",
      "+------+-------+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:=====================================================>  (71 + 1) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "_markDf.withColumn(\"row_number\", row_number().over(win)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8167d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "_markDf = _markDf.withColumn(\"markF\", _markDf['mark'].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f22fec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "winF = Window.partitionBy(\"subject\").orderBy(F.col(\"markF\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e494ad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 29:=============================================>         (82 + 1) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+-----+----------+\n",
      "|  name|subject|mark|markF|row_number|\n",
      "+------+-------+----+-----+----------+\n",
      "|임하나|   Math| 100|100.0|         1|\n",
      "|김갑돌|   Math|98.5| 98.5|         2|\n",
      "|김하나|   Math|  80| 80.0|         3|\n",
      "|김하나|English| 100|100.0|         1|\n",
      "|김갑돌|English|82.3| 82.3|         2|\n",
      "|임하나|English|  70| 70.0|         3|\n",
      "+------+-------+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_markDf.withColumn(\"row_number\", row_number().over(winF)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a259b282",
   "metadata": {},
   "source": [
    "**rank()**\n",
    "- rank() 윈도우 함수는 각 그룹별로 등위를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "848cc9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 69:=============================================>         (83 + 1) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+-----+----+\n",
      "|  name|subject|mark|markF|rank|\n",
      "+------+-------+----+-----+----+\n",
      "|임하나|   Math| 100|100.0|   1|\n",
      "|김갑돌|   Math|98.5| 98.5|   2|\n",
      "|김하나|   Math|  80| 80.0|   3|\n",
      "|김하나|English| 100|100.0|   1|\n",
      "|김갑돌|English|82.3| 82.3|   2|\n",
      "|임하나|English|  70| 70.0|   3|\n",
      "+------+-------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "\n",
    "_markDf.withColumn(\"rank\", rank().over(winF)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2aa0b",
   "metadata": {},
   "source": [
    "### 분석 함수\n",
    "**cume_dist()**\n",
    "- cume_dist() 윈도우 함수는 누적 분포 값을 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23490c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+-----+------------------+\n",
      "|  name|subject|mark|markF|         cume_dist|\n",
      "+------+-------+----+-----+------------------+\n",
      "|임하나|   Math| 100|100.0|0.3333333333333333|\n",
      "|김갑돌|   Math|98.5| 98.5|0.6666666666666666|\n",
      "|김하나|   Math|  80| 80.0|               1.0|\n",
      "|김하나|English| 100|100.0|0.3333333333333333|\n",
      "|김갑돌|English|82.3| 82.3|0.6666666666666666|\n",
      "|임하나|English|  70| 70.0|               1.0|\n",
      "+------+-------+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "\n",
    "_markDf.withColumn(\"cume_dist\", cume_dist().over(winF)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6079927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+-----+----+\n",
      "|  name|subject|mark|markF| lag|\n",
      "+------+-------+----+-----+----+\n",
      "|임하나|   Math| 100|100.0|null|\n",
      "|김갑돌|   Math|98.5| 98.5| 100|\n",
      "|김하나|   Math|  80| 80.0|98.5|\n",
      "|김하나|English| 100|100.0|null|\n",
      "|김갑돌|English|82.3| 82.3| 100|\n",
      "|임하나|English|  70| 70.0|82.3|\n",
      "+------+-------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "\n",
    "_markDf.withColumn(\"lag\", lag(\"mark\", 1).over(winF)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5731cf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+-----+----+\n",
      "|  name|subject|mark|markF|lead|\n",
      "+------+-------+----+-----+----+\n",
      "|임하나|   Math| 100|100.0|98.5|\n",
      "|김갑돌|   Math|98.5| 98.5|  80|\n",
      "|김하나|   Math|  80| 80.0|null|\n",
      "|김하나|English| 100|100.0|82.3|\n",
      "|김갑돌|English|82.3| 82.3|  70|\n",
      "|임하나|English|  70| 70.0|null|\n",
      "+------+-------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "\n",
    "_markDf.withColumn(\"lead\", lead(\"mark\", 1).over(winF)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1a036",
   "metadata": {},
   "source": [
    "### Aggregate Functions\n",
    "- 그룹을 구분할 때 정렬할 필요가 없다. mark가 string으로 설정되어 있을 때, 평균, 합계, 최소, 최대 함수는 어떻게 출력될까\n",
    "\n",
    "- 문제 없이 실행되지만, 결과는 올바르지 않다. 데이터 타입은 항상 주의해야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aaa4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "winAgg = Window.partitionBy(\"subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82ea55a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 109:===============================================>      (88 + 1) / 100]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+-----+-----------------+-----------------+----+-----+\n",
      "|  name|subject|mark|markF|              avg|              sum| min|  max|\n",
      "+------+-------+----+-----+-----------------+-----------------+----+-----+\n",
      "|김하나|   Math|  80| 80.0|92.83333333333333|            278.5|80.0|100.0|\n",
      "|임하나|   Math| 100|100.0|92.83333333333333|            278.5|80.0|100.0|\n",
      "|김갑돌|   Math|98.5| 98.5|92.83333333333333|            278.5|80.0|100.0|\n",
      "|김하나|English| 100|100.0|84.10000101725261|252.3000030517578|70.0|100.0|\n",
      "|임하나|English|  70| 70.0|84.10000101725261|252.3000030517578|70.0|100.0|\n",
      "|김갑돌|English|82.3| 82.3|84.10000101725261|252.3000030517578|70.0|100.0|\n",
      "+------+-------+----+-----+-----------------+-----------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 111:==============================================>        (64 + 2) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_markDf.withColumn(\"avg\", F.avg(F.col(\"markF\")).over(winAgg))\\\n",
    "    .withColumn(\"sum\", F.sum(F.col(\"markF\")).over(winAgg))\\\n",
    "    .withColumn(\"min\", F.min(F.col(\"markF\")).over(winAgg))\\\n",
    "    .withColumn(\"max\", F.max(F.col(\"markF\")).over(winAgg))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cccb6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfcc2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_analysis",
   "language": "python",
   "name": "bigdata_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
