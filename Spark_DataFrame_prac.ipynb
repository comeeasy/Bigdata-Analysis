{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5026341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/02 16:10:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/02 16:10:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "        .builder\\\n",
    "        .master('local')\\\n",
    "        .config(conf=myConf)\\\n",
    "        .appName('dataframe_prac')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f264f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js', 170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee', 170)]\n",
    "myDf = spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f82802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ffdcd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a5fe9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_1='1', _2='kim, js', _3=170)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04b1cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "Person = Row(\"Year\", \"Name\", \"Height\")\n",
    "myRows = [\n",
    "    Person(\"1\", \"Lee\", 175),\n",
    "    Person(\"2\", \"Lim\", 180),\n",
    "    Person(\"2\", \"Kim\", 170)\n",
    "]\n",
    "myDf = spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33366f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5faab8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|Year|Name|Height|\n",
      "+----+----+------+\n",
      "|   1| Lee|   175|\n",
      "|   2| Lim|   180|\n",
      "|   2| Kim|   170|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d7b9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Name</th>\n",
       "      <th>Height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Lee</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Lim</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kim</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Year Name  Height\n",
       "0    1  Lee     175\n",
       "1    2  Lim     180\n",
       "2    2  Kim     170"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b660b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.write.format('com.databricks.spark.csv').save(\"data/_myDf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4771314b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-d3e5d318-6aac-4cb2-82e7-6e180bd5f211-c000.csv\n",
      "1,Lee,175\n",
      "2,Lim,180\n",
      "2,Kim,170\n"
     ]
    }
   ],
   "source": [
    "!ls data/_myDf.csv\n",
    "!cat data/_myDf.csv/part-00000-d3e5d318-6aac-4cb2-82e7-6e180bd5f211-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d4f0186",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.toPandas().to_csv('data/myDf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94f48e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",Year,Name,Height\r\n",
      "0,1,Lee,175\r\n",
      "1,2,Lim,180\r\n",
      "2,2,Kim,170\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/myDf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb67d0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Koread</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HongKong</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Country  codes\n",
       "0  South Koread     81\n",
       "1         Japan     82\n",
       "2      HongKong    852"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "icc = pd.DataFrame({\"Country\": [\"South Koread\", \"Japan\", \"HongKong\"],\n",
    "                   \"codes\": [81, 82, 852]})\n",
    "icc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "433dde44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Koread</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Country  codes\n",
       "0  South Koread     81"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc[icc[\"codes\"] == 81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01ce8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "lines = spark.sparkContext.textFile(\"data/ds_spark_2cols.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94d672d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=35, col2=2),\n",
       " Row(col1=40, col2=27),\n",
       " Row(col1=12, col2=38),\n",
       " Row(col1=15, col2=31),\n",
       " Row(col1=21, col2=1),\n",
       " Row(col1=14, col2=19),\n",
       " Row(col1=46, col2=1),\n",
       " Row(col1=10, col2=34),\n",
       " Row(col1=28, col2=3),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=16, col2=2),\n",
       " Row(col1=30, col2=3),\n",
       " Row(col1=32, col2=2),\n",
       " Row(col1=48, col2=1),\n",
       " Row(col1=31, col2=2),\n",
       " Row(col1=22, col2=1),\n",
       " Row(col1=12, col2=3),\n",
       " Row(col1=39, col2=29),\n",
       " Row(col1=19, col2=37),\n",
       " Row(col1=25, col2=2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll2 = lines.map(lambda l: l.split(\",\"))\\\n",
    "            .map(lambda p: Row(col1=int(p[0].strip()), \n",
    "                               col2=int(p[1].strip())))\n",
    "myDf = spark.createDataFrame(coll2)\n",
    "myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd6b17a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0320c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "    .format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true', delimiter=\",\")\\\n",
    "    .load(\"data/ds_spark.csv\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e674e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "    .options(header='true', inferschma='true', delimiter=',')\\\n",
    "    .csv('data/ds_spark.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484b5fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_analysis",
   "language": "python",
   "name": "bigdata_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
