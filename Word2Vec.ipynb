{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5578b9",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "- 단어를 vector로 변환하는 방법을 말한다.\n",
    "\n",
    "- __Bag Of Words 모델은 단어 순서와 문맥을 무시한다.__\n",
    "\n",
    "- 이러한 단점을 극복하기 위해 __BoW로 부터 단어들 간의 맥락 또는 연관성 Word Embedding을 신경망으로 학습__하여 Word2Vec을 계산한다.\n",
    "\n",
    "- 단어들을 vector로 변환하게 되면 서로 간의 거리를 측정하여 연산이 가능해진다.\n",
    "\n",
    "- vectorSize는 단어 벡터를 몇 개로 구성할지, minCount는 최소 단어 빈도를 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebd2352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/15 09:17:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"w2v\")\\\n",
    "    .config(conf=pyspark.SparkConf())\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d318e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2d=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e408dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = spark.createDataFrame(doc2d, [\"sent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a7b01b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15aac7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/15 09:19:38 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/11/15 09:19:38 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"w2v\")\n",
    "model = word2vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8de88121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([0.0199, -0.0069, -0.0002]))\n",
      "Row(w2v=DenseVector([0.0866, -0.0532, -0.0113]))\n",
      "Row(w2v=DenseVector([0.0298, 0.0029, 0.0186]))\n"
     ]
    }
   ],
   "source": [
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95566461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------+\n",
      "|word    |vector                                                            |\n",
      "+--------+------------------------------------------------------------------+\n",
      "|trouble |[0.08160634338855743,-0.043025512248277664,0.10656099021434784]   |\n",
      "|mother  |[0.14179208874702454,-0.015200603753328323,0.1217670664191246]    |\n",
      "|find    |[-0.01224907673895359,0.026784956455230713,0.10260768979787827]   |\n",
      "|standing|[-0.041652701795101166,0.037779148668050766,0.0017204727046191692]|\n",
      "|wisdom, |[0.10510547459125519,-0.09538917988538742,-0.02657231315970421]   |\n",
      "|in      |[-0.008711651898920536,0.1659986525774002,-0.11379893869161606]   |\n",
      "|myself  |[0.15553179383277893,-0.04055459052324295,-0.01095652673393488]   |\n",
      "|is      |[-0.045858271420001984,0.08375487476587296,-0.10092845559120178]  |\n",
      "|darkness|[0.13663016259670258,0.024674516171216965,-0.006880973465740681]  |\n",
      "|우리    |[-0.15229535102844238,0.12856164574623108,0.12518559396266937]    |\n",
      "|front   |[-0.03369985148310661,-0.13977976143360138,0.11206331849098206]   |\n",
      "|it      |[0.0777786374092102,0.0048708198592066765,-0.004635296296328306]  |\n",
      "|너      |[0.11811912804841995,-0.0858713760972023,-0.11936429142951965]    |\n",
      "|she     |[0.15294910967350006,-0.11004988849163055,0.09802457690238953]    |\n",
      "|comes   |[0.04845762252807617,-0.042694345116615295,-0.04857766255736351]  |\n",
      "|i       |[0.035786036401987076,-0.027627579867839813,0.14095421135425568]  |\n",
      "|hour    |[0.12835094332695007,-0.1429475098848343,-0.05488637834787369]    |\n",
      "|to      |[0.025038544088602066,-0.12940123677253723,0.011153152212500572]  |\n",
      "|speaking|[0.12593373656272888,0.05705349147319794,0.020684439688920975]    |\n",
      "|mary    |[0.09492619335651398,-0.035982534289360046,-0.12743692100048065]  |\n",
      "+--------+------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.getVectors().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600be22d",
   "metadata": {},
   "source": [
    "## NGram\n",
    "\n",
    "- text를 대상으로 하면, n-gram은 연속된 n개의 token으로 구성된 Series를 말한다.\n",
    "\n",
    "- unigram은 한 단어로, bigram은 두 단어로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d068c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDf = ngram.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88ac27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------------------------------------+--------------------------------------------------------------------------+\n",
      "|sent                                  |words                                          |ngrams                                                                    |\n",
      "+--------------------------------------+-----------------------------------------------+--------------------------------------------------------------------------+\n",
      "|When I find myself in times of trouble|[when, i, find, myself, in, times, of, trouble]|[when i, i find, find myself, myself in, in times, times of, of trouble]  |\n",
      "|Mother Mary comes to me               |[mother, mary, comes, to, me]                  |[mother mary, mary comes, comes to, to me]                                |\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, of, wisdom,, let, it, be]    |[speaking words, words of, of wisdom,, wisdom, let, let it, it be]        |\n",
      "|And in my hour of darkness            |[and, in, my, hour, of, darkness]              |[and in, in my, my hour, hour of, of darkness]                            |\n",
      "|She is standing right in front of me  |[she, is, standing, right, in, front, of, me]  |[she is, is standing, standing right, right in, in front, front of, of me]|\n",
      "|Speaking words of wisdom, let it be   |[speaking, words, of, wisdom,, let, it, be]    |[speaking words, words of, of wisdom,, wisdom, let, let it, it be]        |\n",
      "|우리 Let it be                        |[우리, let, it, be]                            |[우리 let, let it, it be]                                                 |\n",
      "|나 Let it be                          |[나, let, it, be]                              |[나 let, let it, it be]                                                   |\n",
      "|너 Let it be                          |[너, let, it, be]                              |[너 let, let it, it be]                                                   |\n",
      "|Let it be                             |[let, it, be]                                  |[let it, it be]                                                           |\n",
      "|Whisper words of wisdom, let it be    |[whisper, words, of, wisdom,, let, it, be]     |[whisper words, words of, of wisdom,, wisdom, let, let it, it be]         |\n",
      "+--------------------------------------+-----------------------------------------------+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90e5b1b",
   "metadata": {},
   "source": [
    "## StringIndexed\n",
    "\n",
    "- string column을 index column으로 변환한다.\n",
    "\n",
    "- __빈도가 높은 순서__대로 0.0부터 index가 주어진다.\n",
    "\n",
    "- index는 double형을 갖게 되며 없는 label에 대해서 예외가 발생할 수 있으므로 (default), setHandleInvalid(\"skip\")과 같은 함수를 사용가능하고 arg로는 \"skip\", \"keep\", \"error\"등으로 설정가능\n",
    "\n",
    "\n",
    "| 구분 | 설명 | 예 |\n",
    "|----|------|-------------|\n",
    "| norminal | 명목 또는 구분 값 category | 사자, 호랑이, 사람 |\n",
    "| ordinal | 명목값과 다른 점은 순서가 있다. | 키 low, mid, high |\n",
    "|interval | 일정한 간격이 있다. | 150-165, 165-180, 180-195 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b14c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c91601ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+---------+\n",
      "|sent                                  |sentLabel|\n",
      "+--------------------------------------+---------+\n",
      "|When I find myself in times of trouble|5.0      |\n",
      "|Mother Mary comes to me               |3.0      |\n",
      "|Speaking words of wisdom, let it be   |0.0      |\n",
      "|And in my hour of darkness            |1.0      |\n",
      "|She is standing right in front of me  |4.0      |\n",
      "|Speaking words of wisdom, let it be   |0.0      |\n",
      "|우리 Let it be                        |9.0      |\n",
      "|나 Let it be                          |7.0      |\n",
      "|너 Let it be                          |8.0      |\n",
      "|Let it be                             |2.0      |\n",
      "|Whisper words of wisdom, let it be    |6.0      |\n",
      "+--------------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = labelIndexer.fit(myDf)\n",
    "siDf = model.transform(myDf)\n",
    "siDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb1738",
   "metadata": {},
   "source": [
    "## One-hot Encoding\n",
    "\n",
    "- 앞서 StringIndexed는 0 < 1 < 2 .. 처럼 순서가 있는 것으로 보여질 수 있다. 그러나 실제로는 순서가 없다.\n",
    "\n",
    "- One-hot encoding은 명목 변수 인덱스를 이진 벡터로 변환하여, 서로 순서가 없도록 한다.\n",
    "\n",
    "| 명목 값 | 이진 벡터 | Sparse Vector |\n",
    "| ----- | ------- | ------------- |\n",
    "|A.     | 10.     | (2, [0], [1.0]) |\n",
    "|B      | 01.     | (2, [1], [1.0]) |\n",
    "|C.     | 00.     | (2, [], [])   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "935bc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, \"B\"),\n",
    "    (2, \"C\"),\n",
    "    (3, \"A\"),\n",
    "    (4, \"B\"),\n",
    "    (5, \"C\"),\n",
    "    (6, \"A\")\n",
    "], [\"id\", \"grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1d084d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"grade\", outputCol=\"gradeIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8006e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"gradeIndex\", outputCol=\"gradeVec\")\n",
    "encoded = encoder.fit(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67dea2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "| id|grade|gradeIndex|     gradeVec|\n",
      "+---+-----+----------+-------------+\n",
      "|  1|    B|       1.0|(2,[1],[1.0])|\n",
      "|  2|    C|       2.0|    (2,[],[])|\n",
      "|  3|    A|       0.0|(2,[0],[1.0])|\n",
      "|  4|    B|       1.0|(2,[1],[1.0])|\n",
      "|  5|    C|       2.0|    (2,[],[])|\n",
      "|  6|    A|       0.0|(2,[0],[1.0])|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded.transform(indexed).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfefc7",
   "metadata": {},
   "source": [
    "##  연속 데이터의 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84c51e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t65.78\t112.99\r\n",
      "2\t71.52\t136.49\r\n",
      "3\t69.40\t153.03\r\n",
      "4\t68.22\t142.34\r\n",
      "5\t67.79\t144.30\r\n",
      "6\t68.70\t123.30\r\n",
      "7\t69.80\t141.49\r\n",
      "8\t70.01\t136.46\r\n",
      "9\t67.90\t112.37\r\n",
      "10\t66.78\t120.67\r\n",
      "11\t66.49\t127.45\r\n",
      "12\t67.62\t114.14\r\n",
      "13\t68.30\t125.61\r\n",
      "14\t67.12\t122.46\r\n",
      "15\t68.28\t116.09\r\n",
      "16\t71.09\t140.00\r\n",
      "17\t66.46\t129.50\r\n",
      "18\t68.65\t142.97\r\n",
      "19\t71.23\t137.90\r\n",
      "20\t67.13\t124.04\r\n",
      "21\t67.83\t141.28\r\n",
      "22\t68.88\t143.54\r\n",
      "23\t63.48\t97.90\r\n",
      "24\t68.42\t129.50\r\n",
      "25\t67.63\t141.85\r\n",
      "26\t67.21\t129.72\r\n",
      "27\t70.84\t142.42\r\n",
      "28\t67.49\t131.55\r\n",
      "29\t66.53\t108.33\r\n",
      "30\t65.44\t113.89\r\n",
      "31\t69.52\t103.30\r\n",
      "32\t65.81\t120.75\r\n",
      "33\t67.82\t125.79\r\n",
      "34\t70.60\t136.22\r\n",
      "35\t71.80\t140.10\r\n",
      "36\t69.21\t128.75\r\n",
      "37\t66.80\t141.80\r\n",
      "38\t67.66\t121.23\r\n",
      "39\t67.81\t131.35\r\n",
      "40\t64.05\t106.71\r\n",
      "41\t68.57\t124.36\r\n",
      "42\t65.18\t124.86\r\n",
      "43\t69.66\t139.67\r\n",
      "44\t67.97\t137.37\r\n",
      "45\t65.98\t106.45\r\n",
      "46\t68.67\t128.76\r\n",
      "47\t66.88\t145.68\r\n",
      "48\t67.70\t116.82\r\n",
      "49\t69.82\t143.62\r\n",
      "50\t69.09\t134.93\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/ds_spark_heightweights.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29604dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "rdd = spark.sparkContext.textFile(os.path.join(\"data\", \"ds_spark_heightweights.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51a8f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd = rdd.map(lambda x: [float(_x) for _x in x.split('\\t')])\n",
    "myDf = spark.createDataFrame(myRdd, [\"id\", \"weight\", \"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "394957a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc8fa6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")\n",
    "binDf = binarizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53cb2998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "|11.0| 66.49|127.45|    0.0|\n",
      "|12.0| 67.62|114.14|    0.0|\n",
      "|13.0|  68.3|125.61|    1.0|\n",
      "|14.0| 67.12|122.46|    0.0|\n",
      "|15.0| 68.28|116.09|    1.0|\n",
      "|16.0| 71.09| 140.0|    1.0|\n",
      "|17.0| 66.46| 129.5|    0.0|\n",
      "|18.0| 68.65|142.97|    1.0|\n",
      "|19.0| 71.23| 137.9|    1.0|\n",
      "|20.0| 67.13|124.04|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "binDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ebaa6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "347b7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "|11.0| 66.49|127.45|    0.0|    1.0|\n",
      "|12.0| 67.62|114.14|    0.0|    0.0|\n",
      "|13.0|  68.3|125.61|    1.0|    1.0|\n",
      "|14.0| 67.12|122.46|    0.0|    0.0|\n",
      "|15.0| 68.28|116.09|    1.0|    0.0|\n",
      "|16.0| 71.09| 140.0|    1.0|    2.0|\n",
      "|17.0| 66.46| 129.5|    0.0|    1.0|\n",
      "|18.0| 68.65|142.97|    1.0|    2.0|\n",
      "|19.0| 71.23| 137.9|    1.0|    2.0|\n",
      "|20.0| 67.13|124.04|    0.0|    1.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qdDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55265e1d",
   "metadata": {},
   "source": [
    "## VectorAssembler\n",
    "\n",
    "- columns를 묶어서 Vector Row로 만든다. feature column을 생성할 경우에 사용한다.\n",
    "\n",
    "- 단 문자열은 묶을 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e30d2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\", \"height3\"], outputCol=\"features\")\n",
    "vaDf = va.transform(qdDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be5e95c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fdb253c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+---------+\n",
      "|  id|weight|height|weight2|height3| features|\n",
      "+----+------+------+-------+-------+---------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|[1.0,0.0]|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|[1.0,2.0]|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|[1.0,1.0]|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|(2,[],[])|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|(2,[],[])|\n",
      "|11.0| 66.49|127.45|    0.0|    1.0|[0.0,1.0]|\n",
      "|12.0| 67.62|114.14|    0.0|    0.0|(2,[],[])|\n",
      "|13.0|  68.3|125.61|    1.0|    1.0|[1.0,1.0]|\n",
      "|14.0| 67.12|122.46|    0.0|    0.0|(2,[],[])|\n",
      "|15.0| 68.28|116.09|    1.0|    0.0|[1.0,0.0]|\n",
      "|16.0| 71.09| 140.0|    1.0|    2.0|[1.0,2.0]|\n",
      "|17.0| 66.46| 129.5|    0.0|    1.0|[0.0,1.0]|\n",
      "|18.0| 68.65|142.97|    1.0|    2.0|[1.0,2.0]|\n",
      "|19.0| 71.23| 137.9|    1.0|    2.0|[1.0,2.0]|\n",
      "|20.0| 67.13|124.04|    0.0|    1.0|[0.0,1.0]|\n",
      "+----+------+------+-------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vaDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c51544",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "- __Pipeline__은 여러 Estimators를 묶은 Estimator를 반환한다.\n",
    "\n",
    "- Pipeline은 여러 작업을 묶어, 순서대로 단계적으로 Estimator를 적용하기 위해 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d41e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "        (0, \"a b c d e spark\", 1.0),\n",
    "        (1, \"b d\", 0.0),\n",
    "        (2, \"spark f g h\", 1.0),\n",
    "        (3, \"hadoop mapreduce\", 0.0),\n",
    "        (4, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "74ff9737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[tokenizer,\n",
    "           hashingTF,\n",
    "           ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_analysis",
   "language": "python",
   "name": "bigdata_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
