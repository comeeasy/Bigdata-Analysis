{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99cb350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/kddcup.data_10_percent.gz doesn't exist\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join('data', 'kddcup.data_10_percent.gz')\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "if (not os.path.exists(_fname)):\n",
    "    print(f\"{_fname} doesn't exist\\nstart download data from {_url}\")\n",
    "    _f = urlretrieve(_url, _fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c01cb",
   "metadata": {},
   "source": [
    "### RDD 생성\n",
    "\n",
    "**RDD** 는 gz와 같은 압축파일에서 데이터를 읽어서 생성할 수 있다.<br>\n",
    "\n",
    "반면, DataFrame은 구조 schema를 정의해야하기 때문에 쉽지 않아. 여기서는 오류가 발생한다.<br>\n",
    "따라서 RDD를 생성하고 난 후, 그로부터 DataFrame을 생성하고, Sql을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a00d30ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/17 16:25:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"Week7-prac\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78dd0594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)\n",
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b73c32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "418f9ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'tcp',\n",
       "  'http',\n",
       "  'SF',\n",
       "  '181',\n",
       "  '5450',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8',\n",
       "  '8',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '9',\n",
       "  '9',\n",
       "  '1.00',\n",
       "  '0.00',\n",
       "  '0.11',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  '0.00',\n",
       "  'normal.']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_allRdd = _rdd.map(lambda x: x.split(','))\n",
    "_allRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe34562",
   "metadata": {},
   "source": [
    "### 정상 공격 건수\n",
    "\n",
    "데이터가 normal인 경우와 아닌 경우로 구분하자.<br>\n",
    "fileter()는 41번째 행을 조건에 따라 데이터를 구분한다.<br>\n",
    "count() 함수로 건수를 계산하면 'normal' 97,278 'attack'은 396,743건이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c45f61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278 396743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "_normalRdd = _allRdd.filter(lambda x: x[41] == \"normal.\")\n",
    "_attackRdd  = _allRdd.filter(lambda x: x[41] != \"normal.\")\n",
    "print(_normalRdd.count(), _attackRdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618f11d7",
   "metadata": {},
   "source": [
    "### attak별 건수\n",
    "\n",
    "attack 종류는 41번째 열에 구분되어 있다. 총 494,021건을 정상 'normal', 나머지는 'attack'으로 구분한다.<br>\n",
    "attack은 크게 4종류로 나눈다. DOS는 서비스 거부, R2L은 원격 침입, U2R은 루트권한침입, probing은 탐지이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec18b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/apache-spark/3.1.2/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('normal.', 97278),\n",
       " ('buffer_overflow.', 30),\n",
       " ('loadmodule.', 9),\n",
       " ('perl.', 3),\n",
       " ('neptune.', 107201),\n",
       " ('smurf.', 280790),\n",
       " ('guess_passwd.', 53),\n",
       " ('pod.', 264),\n",
       " ('teardrop.', 979),\n",
       " ('portsweep.', 1040),\n",
       " ('ipsweep.', 1247),\n",
       " ('land.', 21),\n",
       " ('ftp_write.', 8),\n",
       " ('back.', 2203),\n",
       " ('imap.', 12),\n",
       " ('satan.', 1589),\n",
       " ('phf.', 4),\n",
       " ('nmap.', 231),\n",
       " ('multihop.', 7),\n",
       " ('warezmaster.', 20),\n",
       " ('warezclient.', 1020),\n",
       " ('spy.', 2),\n",
       " ('rootkit.', 10)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "_41.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e66c70c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('normal.', 97278),\n",
       " ('buffer_overflow.', 30),\n",
       " ('loadmodule.', 9),\n",
       " ('perl.', 3),\n",
       " ('neptune.', 107201),\n",
       " ('smurf.', 280790),\n",
       " ('guess_passwd.', 53),\n",
       " ('pod.', 264),\n",
       " ('teardrop.', 979),\n",
       " ('portsweep.', 1040),\n",
       " ('ipsweep.', 1247),\n",
       " ('land.', 21),\n",
       " ('ftp_write.', 8),\n",
       " ('back.', 2203),\n",
       " ('imap.', 12),\n",
       " ('satan.', 1589),\n",
       " ('phf.', 4),\n",
       " ('nmap.', 231),\n",
       " ('multihop.', 7),\n",
       " ('warezmaster.', 20),\n",
       " ('warezclient.', 1020),\n",
       " ('spy.', 2),\n",
       " ('rootkit.', 10)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "_41.groupByKey().mapValues(lambda x: len(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ff145",
   "metadata": {},
   "source": [
    "### DataFrame 생성\n",
    "\n",
    "열 0, 1, 2, 3, 4, 5, 41 을 선별하여 스키마를 정해서 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10c80265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(','))\n",
    "_csvRdd = _csv.map(lambda x:\n",
    "                  Row(\n",
    "                      duration=int(x[0]),\n",
    "                      protocol=x[1],\n",
    "                      service=x[2],\n",
    "                      flag=x[3],\n",
    "                      src_bytes=int(x[4]),\n",
    "                      dst_bytes=int(x[5]),\n",
    "                      attack=x[41]\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39e98ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: long (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- attack: string (nullable = true)\n",
      "\n",
      "+--------+--------+-------+----+---------+---------+-------+\n",
      "|duration|protocol|service|flag|src_bytes|dst_bytes| attack|\n",
      "+--------+--------+-------+----+---------+---------+-------+\n",
      "|       0|     tcp|   http|  SF|      181|     5450|normal.|\n",
      "|       0|     tcp|   http|  SF|      239|      486|normal.|\n",
      "|       0|     tcp|   http|  SF|      235|     1337|normal.|\n",
      "|       0|     tcp|   http|  SF|      219|     1337|normal.|\n",
      "|       0|     tcp|   http|  SF|      217|     2032|normal.|\n",
      "+--------+--------+-------+----+---------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df = spark.createDataFrame(_csvRdd)\n",
    "_df.printSchema()\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ea5c3",
   "metadata": {},
   "source": [
    "### Attack 분류\n",
    "\n",
    "네트워크 침입이 'attack' 또는 'normal' 에 따라 구분해서 attackB 컬럼을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ff22c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "attackUdfd = udf(lambda x: \"normal\" if x == \"normal.\" else \"attack\", StringType())\n",
    "myDf = _df.withColumn(\"attackB\", attackUdfd(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d115d3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: long (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- attack: string (nullable = true)\n",
      " |-- attackB: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d7ecf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classufy41(s):\n",
    "    if s in [\"normal.\"]: return \"normal\"\n",
    "    elif s in [\"back.\", \"land.\", \"neptune.\", \"pod.\", \"smurf.\", \"teardrop.\"]: return \"dos\"\n",
    "    elif s in [\"ftp_write.\", \"guess_passwd.\", \"imap.\", \"multihop.\", \n",
    "                  \"phf.\", \"spy.\", \"warezclient.\", \"warezmaster.\"]: return \"r2l\"\n",
    "    elif s in [\"buffer_overflow.\", \"loadmodule.\", \"perl.\", \"rootkit.\"]: return \"u2r\"\n",
    "    elif s in [\"ipsweep.\", \"nmap.\", \"portsweep.\", \"satan.\"]: return \"probing\"\n",
    "    \n",
    "attack5_udf = udf(classufy41, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "776d4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = myDf.withColumn(\"attack5\", attack5_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddd854e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- duration: long (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- src_bytes: long (nullable = true)\n",
      " |-- dst_bytes: long (nullable = true)\n",
      " |-- attack: string (nullable = true)\n",
      " |-- attackB: string (nullable = true)\n",
      " |-- attack5: string (nullable = true)\n",
      "\n",
      "+--------+--------+-------+----+---------+---------+-------+-------+-------+\n",
      "|duration|protocol|service|flag|src_bytes|dst_bytes| attack|attackB|attack5|\n",
      "+--------+--------+-------+----+---------+---------+-------+-------+-------+\n",
      "|       0|     tcp|   http|  SF|      181|     5450|normal.| normal| normal|\n",
      "|       0|     tcp|   http|  SF|      239|      486|normal.| normal| normal|\n",
      "|       0|     tcp|   http|  SF|      235|     1337|normal.| normal| normal|\n",
      "|       0|     tcp|   http|  SF|      219|     1337|normal.| normal| normal|\n",
      "|       0|     tcp|   http|  SF|      217|     2032|normal.| normal| normal|\n",
      "+--------+--------+-------+----+---------+---------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.1.2/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.1.2/libexec/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.1.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.1.2/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5cad906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|attack5| count|\n",
      "+-------+------+\n",
      "|probing|  4107|\n",
      "|    u2r|    52|\n",
      "| normal| 97278|\n",
      "|    r2l|  1126|\n",
      "|    dos|391458|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:==============================================>         (62 + 1) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "myDf.groupby(\"attack5\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "455561ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|protocol| count|\n",
      "+--------+------+\n",
      "|     tcp|190065|\n",
      "|     udp| 20354|\n",
      "|    icmp|283602|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('protocol').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e73ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|attackB|protocol| count|\n",
      "+-------+--------+------+\n",
      "| normal|     udp| 19177|\n",
      "| normal|    icmp|  1288|\n",
      "| normal|     tcp| 76813|\n",
      "| attack|    icmp|282314|\n",
      "| attack|     tcp|113252|\n",
      "| attack|     udp|  1177|\n",
      "+-------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy(\"attackB\", \"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "473bbeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+-----+\n",
      "|attackB|  icmp|   tcp|  udp|\n",
      "+-------+------+------+-----+\n",
      "| normal|  1288| 76813|19177|\n",
      "| attack|282314|113252| 1177|\n",
      "+-------+------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy(\"attackB\").pivot(\"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0e2da",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "SQL을 사용해보자. 위에 사용했던 _df에서 임시 테이블 _tab을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "642abbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df.registerTempTable(\"_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "230cd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_interations = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _tab\n",
    "    WHERE protocol='tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5088949d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 71:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tcp_interations.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11f24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_analysis",
   "language": "python",
   "name": "bigdata_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
