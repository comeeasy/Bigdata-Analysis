{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42217f1e",
   "metadata": {},
   "source": [
    "# 1.1 성적 데이터로 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f6ddf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "marks=[\n",
    "    \"김하나, English, 100\",\n",
    "    \"김하나, Math, 80\",\n",
    "    \"임하나, English, 70\",\n",
    "    \"임하나, Math, 100\",\n",
    "    \"김갑돌, English, 82.3\",\n",
    "    \"김갑돌, Math, 98.5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc2bf3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/17 19:41:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local')\\\n",
    "    .config(conf=myConf)\\\n",
    "    .appName('AssignmentWeek7')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3555cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Subject: string (nullable = true)\n",
      " |-- Score: double (nullable = true)\n",
      "\n",
      "+------+--------+-----+\n",
      "|  Name| Subject|Score|\n",
      "+------+--------+-----+\n",
      "|김하나| English|100.0|\n",
      "|김하나|    Math| 80.0|\n",
      "|임하나| English| 70.0|\n",
      "|임하나|    Math|100.0|\n",
      "|김갑돌| English| 82.3|\n",
      "|김갑돌|    Math| 98.5|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "scoreRdd = spark.sparkContext.parallelize(marks)\n",
    "scoreRdd = scoreRdd.map(lambda x: x.split(','))\n",
    "scoreRdd = scoreRdd.map(lambda x:\n",
    "                            Row(\n",
    "                                Name=x[0],\n",
    "                                Subject=x[1],\n",
    "                                Score=float(x[2])\n",
    "                            )\n",
    "                       )\n",
    "scoreDf = spark.createDataFrame(scoreRdd)\n",
    "scoreDf.printSchema()\n",
    "scoreDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28887d7e",
   "metadata": {},
   "source": [
    "# 1-2 zscore 컬럼을 생성.\n",
    "\n",
    "- zscore를 계산하려면, 평균과 표준편차를 알아야 한다. <br>\n",
    "\n",
    "- 계산식에 F함수를 직접 사용하면 오류가 발생한다. 따로 평균과 표준편차를 구해서 계산식에서 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa83a669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98810773, -0.72537388, -1.58211469,  0.98810773, -0.5283235 ,\n",
       "        0.85959661])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "PscorePd = scoreDf.toPandas()\n",
    "zscore(np.array(PscorePd[\"Score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6517d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----------+\n",
      "|  Name| Subject|Score|     zscore|\n",
      "+------+--------+-----+-----------+\n",
      "|김하나| English|100.0| 0.90201485|\n",
      "|김하나|    Math| 80.0|-0.66217273|\n",
      "|임하나| English| 70.0| -1.4442666|\n",
      "|임하나|    Math|100.0| 0.90201485|\n",
      "|김갑돌| English| 82.3|-0.48229116|\n",
      "|김갑돌|    Math| 98.5| 0.78470075|\n",
      "+------+--------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "scoreCol = scoreDf.select(\"Score\").toPandas().to_numpy()\n",
    "u = scoreCol.mean()\n",
    "s = scoreCol.std(ddof=1)\n",
    "\n",
    "zscoreUdf = F.udf(lambda x: (x - float(u)) / float(s), FloatType())\n",
    "scoreDf2 = scoreDf.withColumn(\"zscore\", zscoreUdf(scoreDf[\"Score\"]))\n",
    "scoreDf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85107db",
   "metadata": {},
   "source": [
    "# 1-3 cdf 컬럼을 생성.\n",
    "\n",
    "- scipy.stats.norm.cdf() 함수는 데이터타입을 float로 맞추어 주어야 한다.<br>\n",
    "\n",
    "- cdf는 평균=0, 표준편차=1을 기본 값으로 누적확률을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa74467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "cdfUdf = F.udf(lambda x: float(norm.cdf(x)), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23286b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+-----------+-----------+\n",
      "|  Name| Subject|Score|     zscore|        cdf|\n",
      "+------+--------+-----+-----------+-----------+\n",
      "|김하나| English|100.0| 0.90201485|  0.8164755|\n",
      "|김하나|    Math| 80.0|-0.66217273| 0.25393027|\n",
      "|임하나| English| 70.0| -1.4442666|0.074332014|\n",
      "|임하나|    Math|100.0| 0.90201485|  0.8164755|\n",
      "|김갑돌| English| 82.3|-0.48229116| 0.31479958|\n",
      "|김갑돌|    Math| 98.5| 0.78470075|  0.7836855|\n",
      "+------+--------+-----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scoreDf3 = scoreDf2.withColumn(\"cdf\", cdfUdf(scoreDf2[\"zscore\"]))\n",
    "scoreDf3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2bedaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_analysis",
   "language": "python",
   "name": "bigdata_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
