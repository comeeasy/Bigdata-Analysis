{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07d8a33",
   "metadata": {},
   "source": [
    "# S-4: RDD를 사용하여 word vector 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb1e87",
   "metadata": {},
   "source": [
    " ### word count와 똑같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd675a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ds_spark_wiki.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ds_spark_wiki.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb28521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/02 14:54:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "\n",
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "        .builder\\\n",
    "        .master('local')\\\n",
    "        .config(conf=myConf)\\\n",
    "        .appName('s-4')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "253a23f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark', 7),\n",
       " ('apache', 6),\n",
       " ('아파치', 5),\n",
       " ('스파크', 4),\n",
       " ('the', 3),\n",
       " ('an', 2),\n",
       " ('wikipedia', 1),\n",
       " ('is', 1),\n",
       " ('open', 1),\n",
       " ('source', 1),\n",
       " ('cluster', 1),\n",
       " ('computing', 1),\n",
       " ('framework.', 1),\n",
       " ('스파크는', 1),\n",
       " ('오픈', 1),\n",
       " ('소스', 1),\n",
       " ('클러스터', 1),\n",
       " ('컴퓨팅', 1),\n",
       " ('프레임워크이다.', 1),\n",
       " ('originally', 1),\n",
       " ('developed', 1),\n",
       " ('at', 1),\n",
       " ('university', 1),\n",
       " ('of', 1),\n",
       " ('california', 1),\n",
       " (\"berkeley's\", 1),\n",
       " ('amplab', 1),\n",
       " ('codebase', 1),\n",
       " ('was', 1),\n",
       " ('later', 1),\n",
       " ('donated', 1),\n",
       " ('to', 1),\n",
       " ('software', 1),\n",
       " ('foundation', 1),\n",
       " ('which', 1),\n",
       " ('has', 1),\n",
       " ('maintained', 1),\n",
       " ('it', 1),\n",
       " ('since.', 1),\n",
       " ('provides', 1),\n",
       " ('interface', 1),\n",
       " ('for', 1),\n",
       " ('programming', 1),\n",
       " ('entire', 1),\n",
       " ('clusters', 1),\n",
       " ('with', 1),\n",
       " ('implicit', 1),\n",
       " ('data', 1),\n",
       " ('parallelism', 1),\n",
       " ('and', 1),\n",
       " ('fault-tolerance.', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiRdd = spark.sparkContext.textFile(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "words = wikiRdd.flatMap(lambda x: x.split(sep=\" \"))\\\n",
    "               .map(lambda x: (x.lower().rstrip()\\\n",
    "                    .lstrip().rstrip(',').lstrip('.'), 1))\\\n",
    "             .reduceByKey(lambda x, y: x + y)\n",
    "            #.groupByKey().mapValues(sum) # 동일한 결과 \n",
    "                 \n",
    "words.sortBy(lambda x: -x[1]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec469ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_rdd_wordvector.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_rdd_wordvector.py\n",
    "import pyspark\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf = pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession\\\n",
    "            .builder\\\n",
    "            .master('local')\\\n",
    "            .config(conf=myConf)\\\n",
    "            .appName('s-4')\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    wikiRdd = spark.sparkContext.textFile(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "    words = wikiRdd.flatMap(lambda x: x.split(sep=\" \"))\\\n",
    "                   .map(lambda x: (x.lower().rstrip()\\\n",
    "                        .lstrip().rstrip(',').lstrip('.'), 1))\\\n",
    "                 .reduceByKey(lambda x, y: x + y)\n",
    "                #.groupByKey().mapValues(sum) # 동일한 결과 \n",
    "\n",
    "    \n",
    "    print(words.sortBy(lambda x: -x[1]).collect())\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eb6af86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/homebrew/Cellar/apache-spark/3.1.2/libexec/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/02 15:09:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/10/02 15:09:09 INFO SparkContext: Running Spark version 3.1.2\n",
      "21/10/02 15:09:09 INFO ResourceUtils: ==============================================================\n",
      "21/10/02 15:09:09 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/10/02 15:09:09 INFO ResourceUtils: ==============================================================\n",
      "21/10/02 15:09:09 INFO SparkContext: Submitted application: s-4\n",
      "21/10/02 15:09:09 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/10/02 15:09:09 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/10/02 15:09:09 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/10/02 15:09:09 INFO SecurityManager: Changing view acls to: joono\n",
      "21/10/02 15:09:09 INFO SecurityManager: Changing modify acls to: joono\n",
      "21/10/02 15:09:09 INFO SecurityManager: Changing view acls groups to: \n",
      "21/10/02 15:09:09 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/10/02 15:09:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(joono); groups with view permissions: Set(); users  with modify permissions: Set(joono); groups with modify permissions: Set()\n",
      "21/10/02 15:09:09 INFO Utils: Successfully started service 'sparkDriver' on port 50693.\n",
      "21/10/02 15:09:09 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/10/02 15:09:09 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/10/02 15:09:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/10/02 15:09:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/10/02 15:09:09 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/10/02 15:09:09 INFO DiskBlockManager: Created local directory at /private/var/folders/l4/0n86l_692hx8xxqz3fqkk6mm0000gn/T/blockmgr-96548de6-3033-46be-a3ba-6113290a140f\n",
      "21/10/02 15:09:09 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "21/10/02 15:09:09 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/10/02 15:09:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/02 15:09:10 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "21/10/02 15:09:10 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://localhost:4041\n",
      "21/10/02 15:09:10 INFO Executor: Starting executor ID driver on host localhost\n",
      "21/10/02 15:09:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50694.\n",
      "21/10/02 15:09:10 INFO NettyBlockTransferService: Server created on localhost:50694\n",
      "21/10/02 15:09:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/10/02 15:09:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 50694, None)\n",
      "21/10/02 15:09:10 INFO BlockManagerMasterEndpoint: Registering block manager localhost:50694 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 50694, None)\n",
      "21/10/02 15:09:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 50694, None)\n",
      "21/10/02 15:09:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 50694, None)\n",
      "21/10/02 15:09:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/joono/Desktop/joono/Bigdata-Analysis/spark-warehouse').\n",
      "21/10/02 15:09:10 INFO SharedState: Warehouse path is 'file:/Users/joono/Desktop/joono/Bigdata-Analysis/spark-warehouse'.\n",
      "21/10/02 15:09:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 176.1 KiB, free 434.2 MiB)\n",
      "21/10/02 15:09:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KiB, free 434.2 MiB)\n",
      "21/10/02 15:09:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:50694 (size: 27.2 KiB, free: 434.4 MiB)\n",
      "21/10/02 15:09:10 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "21/10/02 15:09:10 INFO FileInputFormat: Total input files to process : 1\n",
      "21/10/02 15:09:11 INFO SparkContext: Starting job: collect at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:22\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Registering RDD 3 (reduceByKey at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:15) as input to shuffle 0\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Got job 0 (collect at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:22) with 1 output partitions\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:22)\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:15), which has no missing parents\n",
      "21/10/02 15:09:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)\n",
      "21/10/02 15:09:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 434.2 MiB)\n",
      "21/10/02 15:09:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:50694 (size: 7.3 KiB, free: 434.4 MiB)\n",
      "21/10/02 15:09:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:15) (first 15 tasks are for partitions Vector(0))\n",
      "21/10/02 15:09:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21/10/02 15:09:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 4529 bytes) taskResourceAssignments Map()\n",
      "21/10/02 15:09:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "21/10/02 15:09:11 INFO HadoopRDD: Input split: file:/Users/joono/Desktop/joono/Bigdata-Analysis/data/ds_spark_wiki.txt:0+573\n",
      "/opt/homebrew/Cellar/apache-spark/3.1.2/libexec/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n",
      "21/10/02 15:09:11 INFO PythonRunner: Times: total = 399, boot = 380, init = 7, finish = 12\n",
      "21/10/02 15:09:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1565 bytes result sent to driver\n",
      "21/10/02 15:09:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 585 ms on localhost (executor driver) (1/1)\n",
      "21/10/02 15:09:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/10/02 15:09:11 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50695\n",
      "21/10/02 15:09:11 INFO DAGScheduler: ShuffleMapStage 0 (reduceByKey at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:15) finished in 0.639 s\n",
      "21/10/02 15:09:11 INFO DAGScheduler: looking for newly runnable stages\n",
      "21/10/02 15:09:11 INFO DAGScheduler: running: Set()\n",
      "21/10/02 15:09:11 INFO DAGScheduler: waiting: Set(ResultStage 1)\n",
      "21/10/02 15:09:11 INFO DAGScheduler: failed: Set()\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at collect at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:22), which has no missing parents\n",
      "21/10/02 15:09:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 10.3 KiB, free 434.2 MiB)\n",
      "21/10/02 15:09:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.2 MiB)\n",
      "21/10/02 15:09:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:50694 (size: 5.9 KiB, free: 434.4 MiB)\n",
      "21/10/02 15:09:11 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1388\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (PythonRDD[6] at collect at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:22) (first 15 tasks are for partitions Vector(0))\n",
      "21/10/02 15:09:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "21/10/02 15:09:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "21/10/02 15:09:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "21/10/02 15:09:11 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks\n",
      "21/10/02 15:09:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/10/02 15:09:11 INFO PythonRunner: Times: total = 5, boot = -164, init = 168, finish = 1\n",
      "21/10/02 15:09:11 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2467 bytes result sent to driver\n",
      "21/10/02 15:09:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 40 ms on localhost (executor driver) (1/1)\n",
      "21/10/02 15:09:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/10/02 15:09:11 INFO DAGScheduler: ResultStage 1 (collect at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:22) finished in 0.048 s\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/10/02 15:09:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "21/10/02 15:09:11 INFO DAGScheduler: Job 0 finished: collect at /Users/joono/Desktop/joono/Bigdata-Analysis/src/ds_rdd_wordvector.py:22, took 0.718611 s\n",
      "[('spark', 7), ('apache', 6), ('아파치', 5), ('스파크', 4), ('the', 3), ('an', 2), ('wikipedia', 1), ('is', 1), ('open', 1), ('source', 1), ('cluster', 1), ('computing', 1), ('framework.', 1), ('스파크는', 1), ('오픈', 1), ('소스', 1), ('클러스터', 1), ('컴퓨팅', 1), ('프레임워크이다.', 1), ('originally', 1), ('developed', 1), ('at', 1), ('university', 1), ('of', 1), ('california', 1), (\"berkeley's\", 1), ('amplab', 1), ('codebase', 1), ('was', 1), ('later', 1), ('donated', 1), ('to', 1), ('software', 1), ('foundation', 1), ('which', 1), ('has', 1), ('maintained', 1), ('it', 1), ('since.', 1), ('provides', 1), ('interface', 1), ('for', 1), ('programming', 1), ('entire', 1), ('clusters', 1), ('with', 1), ('implicit', 1), ('data', 1), ('parallelism', 1), ('and', 1), ('fault-tolerance.', 1)]\n",
      "21/10/02 15:09:11 INFO SparkUI: Stopped Spark web UI at http://localhost:4041\n",
      "21/10/02 15:09:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/10/02 15:09:11 INFO MemoryStore: MemoryStore cleared\n",
      "21/10/02 15:09:11 INFO BlockManager: BlockManager stopped\n",
      "21/10/02 15:09:11 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/10/02 15:09:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/10/02 15:09:11 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/10/02 15:09:12 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/10/02 15:09:12 INFO ShutdownHookManager: Deleting directory /private/var/folders/l4/0n86l_692hx8xxqz3fqkk6mm0000gn/T/spark-af09cc53-63fb-442e-b031-5ea3ee844e8a\n",
      "21/10/02 15:09:12 INFO ShutdownHookManager: Deleting directory /private/var/folders/l4/0n86l_692hx8xxqz3fqkk6mm0000gn/T/spark-af09cc53-63fb-442e-b031-5ea3ee844e8a/pyspark-7be1fb20-7fa4-4c50-ba3e-14175889e98f\n",
      "21/10/02 15:09:12 INFO ShutdownHookManager: Deleting directory /private/var/folders/l4/0n86l_692hx8xxqz3fqkk6mm0000gn/T/spark-dca99a12-4603-4c23-b057-e2077a04e870\n"
     ]
    }
   ],
   "source": [
    "!spark-submit src/ds_rdd_wordvector.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata_analysis",
   "language": "python",
   "name": "bigdata_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
